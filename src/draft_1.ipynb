{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA 1030 Project\n",
    "\n",
    "- UCI SECOM Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score,recall_score,roc_auc_score\n",
    "\n",
    "\n",
    "# models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1567, 592)\n"
     ]
    }
   ],
   "source": [
    "# reading the data\n",
    "data = pd.read_csv('../data/uci-secom.csv')\n",
    "\n",
    "# we have 1,567 rows and 592 columns\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparatinon \n",
    "\n",
    "- **Imbalanced data**: 93.4% (Pass) / 6.6% (Fail)\n",
    "\n",
    "- **Univariate features**: 116 (dropped before split)\n",
    "\n",
    "- **CV**: K-fold / Stratified-K-Fold\n",
    "\n",
    "- **Standardization**: StandardScalar (All numerical values)\n",
    "\n",
    "- **Missing Data Imputation**:  Since the absence of a signal (the feature value) is assumed to be no signal, here we replace the null valeus with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape after dropping univariate columns: (1567, 480)\n",
      "X shape: (1567, 478)\n"
     ]
    }
   ],
   "source": [
    "# 1. Missing data: replace with 0\n",
    "data = data.replace(np.NaN, 0)\n",
    "\n",
    "# 2. Drop Unique Value Columns\n",
    "unique_value_columns = data.columns[data.nunique() == 1]\n",
    "data_cleaned = data.drop(columns=unique_value_columns) \n",
    "print(\"Data Shape after dropping univariate columns:\", data_cleaned.shape)\n",
    "\n",
    "# 3. Select X,Y data\n",
    "y = data_cleaned['Pass/Fail']\n",
    "X = data_cleaned.drop(columns=['Pass/Fail','Time'])\n",
    "print(\"X shape:\",X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML pipeline Function\n",
    "\n",
    "1. Split: Other/Test (80/20), with stratify = y\n",
    "2. CV: Kfolds (K=4)\n",
    "3. Evaluation Metrics: F-1 Score\n",
    "    - This is due to the imblanced dataset, accuracy is not proper to use here\n",
    "    - Recall, and BER (balanced error rate) will also be calculated\n",
    "\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import f1_score, make_scorer,accuracy_score\n",
    "\n",
    "def MLpipe_KFold_f1_score(X, y, preprocessor, ML_algo, param_grid):\n",
    "    '''\n",
    "    This function splits the data into other/test (80/20) and then applies KFold with 4 folds to 'other'.\n",
    "    The F1 score is maximized through cross-validation during grid search.\n",
    "    '''\n",
    "\n",
    "    # Lists to be returned\n",
    "    test_scores = []\n",
    "    f1_scores = []\n",
    "    best_models = []\n",
    "\n",
    "    # Define random states for reproducibility\n",
    "    random_states = range(5)\n",
    "\n",
    "    for random_state in random_states:\n",
    "        # Split the data while maintaining class distribution - use stratify = y\n",
    "        X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "\n",
    "        # Preprocess the data\n",
    "        X_other_prep = preprocessor.fit_transform(X_other)\n",
    "        X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "        # Create the model with GridSearchCV using F1-score as the scoring metric\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=ML_algo,\n",
    "            param_grid=param_grid,\n",
    "            cv=KFold(n_splits=4, shuffle=True, random_state=random_state),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit the model using GridSearchCV\n",
    "        grid_search.fit(X_other_prep, y_other)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred = grid_search.predict(X_test_prep)\n",
    "\n",
    "        # Calculate the F1-score on the test set\n",
    "        test_score = accuracy_score(y_test, y_test_pred)\n",
    "        test_f1_scores = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "\n",
    "        # Append the test score and best model\n",
    "        test_scores.append(test_score)\n",
    "        f1_scores.append(test_f1_scores)\n",
    "        best_models.append(grid_search.best_estimator_)\n",
    "\n",
    "    return test_scores, f1_scores, best_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scores across different random states: [0.9299363057324841, 0.9363057324840764, 0.9235668789808917, 0.9299363057324841, 0.9299363057324841]\n",
      "test f1 Scores across different random states: [0.0, 0.09090909090909091, 0.0, 0.0, 0.08333333333333333]\n",
      "Average Test Score: 0.9299363057324841\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the modified function\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Stadardize the data\n",
    "preprocessor = StandardScaler()\n",
    "\n",
    "# Test with logistic regression\n",
    "ML_algo = LogisticRegression()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Call the modified function\n",
    "test_scores, test_f1_scores, best_models = MLpipe_KFold_f1_score(X, y, preprocessor, ML_algo, param_grid)\n",
    "\n",
    "# Analyze the results\n",
    "print(\"Test Scores across different random states:\", test_scores)\n",
    "print(\"test f1 Scores across different random states:\", test_f1_scores)\n",
    "print(\"Average Test Score:\", sum(test_scores) / len(test_scores))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Models: [LogisticRegression(C=0.1, penalty='l1', solver='liblinear'), LogisticRegression(C=0.1, penalty='l1', solver='liblinear'), LogisticRegression(C=0.1, penalty='l1', solver='liblinear'), LogisticRegression(C=0.1, penalty='l1', solver='liblinear'), LogisticRegression(C=0.1, penalty='l1', solver='liblinear')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Models:\", best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import f1_score, recall_score, balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MLpipe_KFold_f1_score(X, y, preprocessor, ML_algo, param_grid, method_name):\n",
    "    '''\n",
    "    This function splits the data into other/test (80/20) and then applies KFold with 4 folds to 'other'.\n",
    "    The F1 score is maximized through cross-validation during grid search.\n",
    "    It returns test accuracy scores, f1 scores, recall scores, BER scores, and best models.\n",
    "    Additionally, it plots the confusion matrix for the best model.\n",
    "    '''\n",
    "    # Lists to be returned\n",
    "    test_scores = []\n",
    "    f1_scores = []\n",
    "    recall_scores = []\n",
    "    ber_scores = []\n",
    "    best_models = []\n",
    "    best_model_index = None\n",
    "    highest_test_f1_score = -np.inf\n",
    "\n",
    "    # Define random states for reproducibility\n",
    "    random_states = range(5)\n",
    "\n",
    "    for idx, random_state in enumerate(random_states):\n",
    "        # Split the data while maintaining class distribution - use stratify=y\n",
    "        X_other, X_test, y_other, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "        )\n",
    "\n",
    "        # Preprocess the data\n",
    "        X_other_prep = preprocessor.fit_transform(X_other)\n",
    "        X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "        # Define the scoring metric for GridSearchCV\n",
    "        f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "        # Create the model with GridSearchCV using F1-score as the scoring metric\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=ML_algo,\n",
    "            param_grid=param_grid,\n",
    "            scoring=f1_scorer,\n",
    "            cv=KFold(n_splits=4, shuffle=True, random_state=random_state),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit the model using GridSearchCV\n",
    "        grid_search.fit(X_other_prep, y_other)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred = grid_search.predict(X_test_prep)\n",
    "\n",
    "        # Calculate evaluation metrics on the test set\n",
    "        test_f1_score = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "        recall = recall_score(y_test, y_test_pred, pos_label=1)\n",
    "        ber = 1 - balanced_accuracy_score(y_test, y_test_pred)  # BER = 1 - balanced accuracy\n",
    "\n",
    "        # Append the test scores and best model\n",
    "        test_scores.append(test_f1_score)\n",
    "        f1_scores.append(test_f1_score)\n",
    "        recall_scores.append(recall)\n",
    "        ber_scores.append(ber)\n",
    "        best_models.append(grid_search.best_estimator_)\n",
    "\n",
    "        # Keep track of the best model based on test F1-score\n",
    "        if test_f1_score > highest_test_f1_score:\n",
    "            highest_test_f1_score = test_f1_score\n",
    "            best_model_index = idx\n",
    "            best_random_state = random_state\n",
    "            best_y_test = y_test\n",
    "            best_y_pred = y_test_pred\n",
    "\n",
    "    # Plot confusion matrix for the best model\n",
    "    cm = confusion_matrix(best_y_test, best_y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(f\"Confusion Matrix for {method_name} (Best Model)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Return the evaluation metrics and best models\n",
    "    return test_scores, f1_scores, recall_scores, ber_scores, best_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ml_methods = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(max_iter=1000),\n",
    "        'param_grid': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'solver': ['liblinear'],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "    },\n",
    "    'SVC':{\n",
    "        'model': SVC(),\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30],\n",
    "            'learning_rate': [0.01, 0.1, 1],\n",
    "            'scale_pos_weight': [1, 10, 100]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
