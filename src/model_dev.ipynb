{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semiconductor Manufacturing - Test Result Classification Project\n",
    "\n",
    "## Model Development\n",
    "\n",
    "This notebook contains Model Development, and save the result to /results folder.\n",
    "\n",
    "Model used: Logistic Regression / Support Vector Classifier / Random Forest Classifer / XGBoost Classifier\n",
    "\n",
    "ML pipeline structure: \n",
    "\n",
    "- Data Preprocess\n",
    "- Standardization\n",
    "\n",
    "- For i in random states, \n",
    "\n",
    "    - Stratifed train/test split\n",
    "    - Stratifed 3 folds CV insider GridSearch CV\n",
    "      - Optimized for F1.5\n",
    "  \n",
    "    - Save mean/std scores\n",
    "    - Hyperpamater Tunning\n",
    "  \n",
    "- Output 5 best models for each random states for each ML algo -> /results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data prep\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV,StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, f1_score, recall_score, roc_auc_score, accuracy_score,fbeta_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# models\n",
    "import joblib\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source Description:\n",
    "\n",
    "- Data: **UCI SECOM Dataset (Kaggle)**\n",
    "\n",
    "    - Data from a semi-conductor manufacturing process\n",
    "\n",
    "    - Number of Instances: 1567\n",
    "\n",
    "    - Number of Attributes: 591\n",
    "    \n",
    "    - Missing Values? Yes\n",
    "\n",
    "- UCI Link: https://archive.ics.uci.edu/dataset/179/secom\n",
    "  \n",
    "- Kaggle Link: https://www.kaggle.com/datasets/paresh2047/uci-semcom\n",
    "\n",
    "- Associated Tasks: **Classification**\n",
    "\n",
    "- **Imbalanced data**: 93.4% (Pass) / 6.6% (Fail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "data = pd.read_csv('../data/uci-secom.csv')\n",
    "\n",
    "# we have 1,567 rows and 592 columns\n",
    "# print(data.shape)\n",
    "\n",
    "# checking the first 5 rows\n",
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocess\n",
    "\n",
    "1. **Drop Univariate features**: \n",
    "\n",
    "    - Drop 116 features that contain constant value (dropped before split)\n",
    "\n",
    "2. **Drop the features with strong correlation**\n",
    "\n",
    "    - Drop features that have strong correaltion with other feature more than absolute value 0.9\n",
    "\n",
    "3. **Replace Missing Data with 0**:  \n",
    "\n",
    "    - According to the data source provider, the absence of a signal (the feature value) is assumed to be **NO** signal, here I replace the null valeus with 0.  \n",
    "\n",
    "    - Only replace missing data with 0 while use RF, Logistic, and SVC.\n",
    "\n",
    "        - Note: For XGBoost, I'll leave it to the model itself the handle the missing values. \n",
    "        \n",
    "4. **Feature Selection**: \n",
    "\n",
    "    - Use built-in feature selection: XGBoost, RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape after dropping univariate columns: (1567, 476)\n"
     ]
    }
   ],
   "source": [
    "# 1. Drop Unique Value Columns\n",
    "unique_value_columns = data.columns[data.nunique() == 1]\n",
    "data_cleaned = data.drop(columns=unique_value_columns) \n",
    "print(\"Data Shape after dropping univariate columns:\", data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data shape after dropping strongly correlated features: (1567, 256)\n"
     ]
    }
   ],
   "source": [
    "# 2. Drop the features have correlation with other feaures more than 0.9\n",
    "corr_matrix = data_cleaned.iloc[:, 1:].corr()\n",
    "strong_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i + 1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "            strong_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "# Drop features with strong correlation (|corr| > 0.9)\n",
    "features_to_drop = set()\n",
    "for feature1, feature2, corr_value in strong_corr_pairs:\n",
    "    if abs(corr_value) > 0.9:\n",
    "        features_to_drop.add(feature2)\n",
    "\n",
    "data_cleaned = data_cleaned.drop(columns=features_to_drop)\n",
    "print(\"New data shape after dropping strongly correlated features:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Replace missing values with '0' for RF, Logistic, and SVC\n",
    "df_na_0 = data_cleaned.replace(np.NaN, 0) # for RF, Logistic, and SVC\n",
    "df_na = data_cleaned # for XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X shape: (1567, 254)\n"
     ]
    }
   ],
   "source": [
    "# 4. Select X,Y data\n",
    "y = data_cleaned['Pass/Fail']\n",
    "y = np.where(y == -1, 0, 1) # change -1 to 0\n",
    "X_0 = df_na_0.drop(columns=['Pass/Fail','Time'])\n",
    "X_XGB = df_na.drop(columns=['Pass/Fail','Time']) # for XGboost\n",
    "print(\"Final X shape:\",X_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Pipeline Function\n",
    "\n",
    "1. **Split & Cross Validation Strategy**:\n",
    "\n",
    "    - This function splits the data into other/test (80/20)\n",
    "  \n",
    "    - KFold with 3 folds to 'other'.\n",
    "\n",
    "2. **Evaluation Metrics**: \n",
    "\n",
    "    - Use F_1.5 Score: The F_1.5 score is maximized through cross-validation during grid search.\n",
    "  \n",
    "      - Why F_1.5: More weights towards recall, consider failture detection matters more in this situation. \n",
    "\n",
    "3. **Function Parameters**:\n",
    "\n",
    "    - X: Features dataframe\n",
    "\n",
    "    - y: Target variable\n",
    "\n",
    "    - ML_algo: Model to train\n",
    "\n",
    "    - param_grid: Hyperparameter grid for GridSearchCV \n",
    "        \n",
    "4. **Returns**:\n",
    "\n",
    "    - ml_results: Contain best model information (*iter_results*) for 5 iteration for one specific ML algo and parameter grid.\n",
    "  \n",
    "    - iter_results: \n",
    "        ```\n",
    "            iter_results['grid'] = grid\n",
    "            iter_results['best_params'] = grid.best_params_\n",
    "            iter_results['validation_score'] = grid.best_score_\n",
    "            iter_results['test_scores'] = fbeta_score(y_test, y_test_pred,beta=1.5, pos_label=1)\n",
    "            iter_results['y_test'] = y_test\n",
    "            iter_results['y_test_pred'] = y_test_pred\n",
    "            iter_results['cv_results'] = grid.cv_results_\n",
    "        ```\n",
    "1. **Reproducibility and Randomness**:\n",
    "\n",
    "    - Random States: Range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_StratifiedKFold_f_score(X, y, ML_algo, param_grid):\n",
    "\n",
    "    ml_results = []\n",
    "    final_models = []   \n",
    "\n",
    "    random_states = range(5) # Random states for reproducibility\n",
    "    for random_state in random_states:\n",
    "        iter_results = {'random_state': random_state}        \n",
    "        print(\"Random State:\", random_state + 1) \n",
    "\n",
    "        X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=random_state)\n",
    "\n",
    "        steps = [('scaler', StandardScaler())]\n",
    "        steps.append(('model', ML_algo))\n",
    "        pipe = Pipeline(steps)\n",
    "        score = make_scorer(fbeta_score,beta=1.5, pos_label=1)\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid, cv=skf, scoring= score, return_train_score=True, verbose=True, n_jobs=-1)\n",
    "        grid.fit(X_other, y_other)\n",
    "\n",
    "        iter_results['grid'] = grid\n",
    "        iter_results['best_params'] = grid.best_params_\n",
    "        iter_results['validation_score'] = grid.best_score_\n",
    "\n",
    "        final_models.append(grid)\n",
    "        y_test_pred = grid.best_estimator_.predict(X_test)\n",
    "        y_test_pred = final_models[-1].predict(X_test)\n",
    "        iter_results['test_scores'] = fbeta_score(y_test, y_test_pred,beta=1.5, pos_label=1)\n",
    "        iter_results['y_test'] = y_test\n",
    "        iter_results['y_test_pred'] = y_test_pred\n",
    "        iter_results['cv_results'] = grid.cv_results_\n",
    "        ml_results.append(iter_results)\n",
    "\n",
    "        print(\"Best Model Parameters:\", iter_results['best_params'])\n",
    "        print('validation score:',iter_results['validation_score'])\n",
    "        print(\"Test F1.5-score:\", iter_results['test_scores'])\n",
    "        \n",
    "    return ml_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Hyperparameter Tunning\n",
    "\n",
    "- Logistic Regression\n",
    "- SVC\n",
    "- Random Forest Classifier\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 1\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data1030/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: {'model__C': 0.01, 'model__l1_ratio': 0.7, 'model__penalty': 'elasticnet', 'model__solver': 'saga'}\n",
      "validation score: 0.310185169150392\n",
      "Test F1.5-score: 0.2765957446808511\n",
      "Random State: 2\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data1030/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: {'model__C': 0.01, 'model__l1_ratio': 0.7, 'model__penalty': 'elasticnet', 'model__solver': 'saga'}\n",
      "validation score: 0.30145158118948445\n",
      "Test F1.5-score: 0.3797752808988764\n",
      "Random State: 3\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data1030/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: {'model__C': 0.01, 'model__l1_ratio': 0.7, 'model__penalty': 'elasticnet', 'model__solver': 'saga'}\n",
      "validation score: 0.2811287267115584\n",
      "Test F1.5-score: 0.3603411513859275\n",
      "Random State: 4\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Model Parameters: {'model__C': 0.01, 'model__l1_ratio': 0.5, 'model__penalty': 'elasticnet', 'model__solver': 'saga'}\n",
      "validation score: 0.27369263989300846\n",
      "Test F1.5-score: 0.2392638036809816\n",
      "Random State: 5\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Model Parameters: {'model__C': 0.01, 'model__penalty': 'l1', 'model__solver': 'liblinear'}\n",
      "validation score: 0.29093883467154485\n",
      "Test F1.5-score: 0.2914798206278027\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression parameter grid \n",
    "lr_param_grid = [\n",
    "    {\n",
    "        'model__solver': ['liblinear'],  #  Only Support l1 & l2\n",
    "        'model__penalty': ['l1', 'l2'], \n",
    "        'model__C': [0.01, 0.1, 1, 10, 100] \n",
    "    },\n",
    "    {\n",
    "        'model__solver': ['saga'],  # Support elastic net\n",
    "        'model__penalty': ['elasticnet'],  \n",
    "        'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'model__l1_ratio': [0.5, 0.7]  # Elastic net ratio\n",
    "    }]\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=100000, class_weight='balanced')\n",
    "\n",
    "lr_results = ML_StratifiedKFold_f_score(X_0, y, lr_model, lr_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/lr_results.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(lr_results, '../results/lr_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 1\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Model Parameters: {'model__C': 1, 'model__gamma': 0.001}\n",
      "validation score: 0.25761848109349733\n",
      "Test F1.5-score: 0.2869757174392936\n",
      "Random State: 2\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Model Parameters: {'model__C': 1, 'model__gamma': 0.001}\n",
      "validation score: 0.2612216093747098\n",
      "Test F1.5-score: 0.24940047961630696\n",
      "Random State: 3\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Model Parameters: {'model__C': 0.01, 'model__gamma': 0.001}\n",
      "validation score: 0.1873506159220445\n",
      "Test F1.5-score: 0.18892733564013842\n",
      "Random State: 4\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Model Parameters: {'model__C': 1, 'model__gamma': 0.001}\n",
      "validation score: 0.24175515305130857\n",
      "Test F1.5-score: 0.3341902313624679\n",
      "Random State: 5\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Model Parameters: {'model__C': 1, 'model__gamma': 0.001}\n",
      "validation score: 0.218758296246775\n",
      "Test F1.5-score: 0.327455919395466\n"
     ]
    }
   ],
   "source": [
    "svc_param_grid = [{\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'model__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "svc_model = SVC(probability=True, class_weight='balanced')\n",
    "\n",
    "svc_results = ML_StratifiedKFold_f_score(X_0, y, svc_model, svc_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/svc_results.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svc_results, '../results/svc_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 1\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best Model Parameters: {'model__max_depth': 2, 'model__max_features': 0.5}\n",
      "validation score: 0.2824915983879206\n",
      "Test F1.5-score: 0.3302540415704388\n",
      "Random State: 2\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best Model Parameters: {'model__max_depth': 2, 'model__max_features': 0.75}\n",
      "validation score: 0.2361911755954796\n",
      "Test F1.5-score: 0.41379310344827586\n",
      "Random State: 3\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best Model Parameters: {'model__max_depth': 2, 'model__max_features': 0.75}\n",
      "validation score: 0.24690804600493868\n",
      "Test F1.5-score: 0.3117505995203837\n",
      "Random State: 4\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best Model Parameters: {'model__max_depth': 2, 'model__max_features': 0.75}\n",
      "validation score: 0.26107174372418684\n",
      "Test F1.5-score: 0.2805755395683453\n",
      "Random State: 5\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best Model Parameters: {'model__max_depth': 2, 'model__max_features': 0.25}\n",
      "validation score: 0.28430317151797996\n",
      "Test F1.5-score: 0.3170731707317073\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    'model__max_depth': [ 2, 3, 4,  5, 10, 100 ],\n",
    "    'model__max_features': [0.25, 0.5, 0.75, 1]}\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "rf_results = ML_StratifiedKFold_f_score(X_0, y, rf_model, rf_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/rf_results.pkl']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_results, '../results/rf_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 1\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Best Model Parameters: {'model__colsample_bytree': 0.5, 'model__learning_rate': 0.01, 'model__max_depth': 1, 'model__subsample': 0.5}\n",
      "validation score: 0.32369811566415657\n",
      "Test F1.5-score: 0.3023255813953488\n",
      "Random State: 2\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Best Model Parameters: {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.1, 'model__max_depth': 1, 'model__subsample': 0.9}\n",
      "validation score: 0.32334400456116735\n",
      "Test F1.5-score: 0.3797752808988764\n",
      "Random State: 3\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Best Model Parameters: {'model__colsample_bytree': 0.9, 'model__learning_rate': 0.01, 'model__max_depth': 1, 'model__subsample': 0.7}\n",
      "validation score: 0.2949882939889226\n",
      "Test F1.5-score: 0.3443708609271523\n",
      "Random State: 4\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Best Model Parameters: {'model__colsample_bytree': 0.5, 'model__learning_rate': 0.01, 'model__max_depth': 1, 'model__subsample': 0.7}\n",
      "validation score: 0.31723329131193573\n",
      "Test F1.5-score: 0.2869757174392936\n",
      "Random State: 5\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Best Model Parameters: {'model__colsample_bytree': 0.9, 'model__learning_rate': 0.01, 'model__max_depth': 2, 'model__subsample': 0.7}\n",
      "validation score: 0.33145009416195853\n",
      "Test F1.5-score: 0.2880886426592798\n"
     ]
    }
   ],
   "source": [
    "negative_instances = len(y) - sum(y) \n",
    "positive_instances = sum(y) \n",
    "scale_pos_weight = negative_instances / positive_instances\n",
    "\n",
    "# XGBoost parameter grid\n",
    "xgb_param_grid = {\n",
    "    'model__max_depth': [1, 2, 3, 5, 10],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.3, 1],\n",
    "    'model__subsample': [0.5, 0.7, 0.9],\n",
    "    'model__colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "xgb_model = XGBClassifier(scale_pos_weight=scale_pos_weight,eval_metric='logloss')\n",
    "\n",
    "xgb_results = ML_StratifiedKFold_f_score(X_XGB, y, xgb_model, xgb_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/xgb_results.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_results, '../results/xgb_results.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
